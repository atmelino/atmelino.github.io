<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html;
      charset=windows-1252">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link type="text/css" href="../../css/blog.css" media="all" rel="stylesheet">
  <title>atmelino</title>
</head>

<body>

  <div class="header">
    <table id="headerline_table">
      <tr>
        <td width="200">
          <a href="../../index.html"><img src="../../images/home.png" alt="home" height="60"></a>
          <a href="../../blogs/index.html"><img src="../../images/scroll.png" alt="scroll" height="60"></a>
          <a href="../../blogs/AI_learning/index.html"><img src="../../images/AI_01.png" alt="deno" height="60"></a>
        </td>
        <td>
          <h2>AI learning blog August 2025 </h2>
        </td>
      </tr>
    </table>
  </div>


  <div class="row">
    <div class="leftcolumn">


      <div class="card">
        <h2>August 2, 2025</h2>
        <h5>NLP transfer learning</h5>


        Word Embeddings in NLP: An Introduction<br>
        <a href="https://hunterheidenreich.com/posts/intro-to-word-embeddings/">
          https://hunterheidenreich.com/posts/intro-to-word-embeddings/ </a><br>
        <br>
        Distributional semantics<br>
        <a href="https://en.wikipedia.org/wiki/Distributional_semantics">
          https://en.wikipedia.org/wiki/Distributional_semantics
        </a><br>
        distributional hypothesis: linguistic items with similar distributions have similar meanings.<br>
        <br>
        word2vec:<br>
        Tool for computing continuous distributed representations of words.<br>
        <a href="https://code.google.com/archive/p/word2vec/">
          https://code.google.com/archive/p/word2vec/
        </a><br>
        <a href="https://www.tensorflow.org/text/tutorials/word2vec">
          https://www.tensorflow.org/text/tutorials/word2vec </a><br>
        <a href="https://en.wikipedia.org/wiki/Word2vec">
          https://en.wikipedia.org/wiki/Word2vec
        </a><br>
        Preservation of semantic and syntactic relationships:<br>
        <img src="images/Word_vector_illustration.png" alt="Word_vector_illustration.png" height="300">
        <br>
        Get training data by extracting text from Wikipedia<br>
        <a href="https://mattmahoney.net/dc/textdata.html">
          https://mattmahoney.net/dc/textdata.html </a><br>

        <br>
        <br>

        <h5>Part 9.3: Transfer Learning for NLP with Keras</h5>
        The code loads an pretrained embedding model as variable model.<br>

        Then it creates an embedding layer as a Keras layer with the parameters from this model,
        and that has an output shape of 20.<br>
        <div class="code">
          hub_layer = hub.KerasLayer(<br>
          model,<br>
          output_shape=[20],<br>
          input_shape=[],<br>
          dtype=tf.string,<br>
          trainable=True<br>
          )
        </div>

        The embedding layer can convert each of the reviews into a 20 number vector.<br>
        For example:<br>
        <div class="code">
          print(hub_layer(train_examples[:1]))
        </div>
        prints<br>
        <div class="output">
          tf.Tensor(
          [[ <br>
          1.7657859 -3.882232 3.913424 -1.5557289 -3.3362343 -1.7357956<br>
          -1.9954445 1.298955 5.081597 -1.1041285 -2.0503852 -0.7267516<br>
          -0.6567596 0.24436145 -3.7208388 2.0954835 2.2969332 -2.0689783<br>
          -2.9489715 -1.1315986 <br>
          ]], shape=(1, 20), dtype=float32)
        </div>
        regardless of how many words the input has.

        <a href=""> </a><br>
        <a href=""> </a><br>
      </div>



      <div class="card">
        <h2>August 3, 2025</h2>

        The model used in 9.3 has its desctription at<br>

        <a href="https://www.kaggle.com/models/google/gnews-swivel/code">
          https://www.kaggle.com/models/google/gnews-swivel/code </a><br>

        "This module .. maps from text to 20-dimensional embedding vectors."<br>

      </div>

      <div class="card">
        <h2>August 5, 2025</h2>
        Word Embedding using Universal Sentence Encoder in Python<br>
        <a href="https://www.geeksforgeeks.org/python/word-embedding-using-universal-sentence-encoder-in-python/">
          https://www.geeksforgeeks.org/python/word-embedding-using-universal-sentence-encoder-in-python/ </a><br>
        <br>
        How to load TF hub model from local system<br>
        <a href="https://stackoverflow.com/questions/60578801/how-to-load-tf-hub-model-from-local-system">
          https://stackoverflow.com/questions/60578801/how-to-load-tf-hub-model-from-local-system </a><br>

        <br>
        https://xianbao-qian.medium.com/how-to-run-tf-hub-locally-without-internet-connection-4506b850a915
        <br>
        <br>
        where does tensorflow_hub store model on ubuntu?<br>
        <br>

        <div class="code">
          module_url
          ="https://www.kaggle.com/models/google/universal-sentence-encoder/tensorFlow2/universal-sentence-encoder/2?tfhub-redirect=true"
          print(hub.resolve(module_url))
        </div>
        prints<br>
        <div class="code">
          /tmp/tfhub_modules/3bdf4002a346590d64dd2aee920834f58917f372
        </div>
        <br>
        <a href="https://www.tensorflow.org/hub/caching">
          https://www.tensorflow.org/hub/caching </a><br>
        <br>


        <h5>Tutorial: Universal Sentence Encoder</h5>

        <a href="https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder">
          https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder </a><br>
        <br>
        The tutorial compares similarity scores computed using sentence embeddings align with human judgements.<br>

        STS Benchmark<br>

        <a href="http://ixa2.si.ehu.es/stswiki">http://ixa2.si.ehu.es/stswiki </a><br>
        <br>
        The benchmark dataset is downloaded:<br>
        <div class="code">
          sts_dataset = tf.keras.utils.get_file(<br>
          fname="Stsbenchmark.tar.gz",<br>
          origin="http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz",<br>
          extract=True)
        </div>
        The folder location of the downloaded dataset is at<br>
        <div class="code">
          ~/.keras/datasets/stsbenchmark
        </div>
        This folder contains a readme.txt.<br>
        "The benchmark comprises 8628 sentence pairs."<br>
        <br>

        csv files<br>
        column headers:<br>
        genre filename year score sentence1 sentence2<br>









      </div>



      <div class="card">
        <h2>Date</h2>
        <a href=""> </a><br>
      </div>




    </div>

    <div class="rightcolumn">
      <div class="card">
        <h3>Posts by Date</h3>
        <a href="AI_learning_2023_02_feb.html">
          AI_learning_2023_02_feb.html</a><br>
        <a href="AI_learning_2023_03_mar.html">
          AI_learning_2023_03_mar.html</a><br>
        <br>
      </div>

      <div class="card">
        <h3>Follow Me</h3>
        <a href="https://discord.gg/F7KQySEW"><img src="../../images/discord.png" alt="discord" height="30"></a>
      </div>

    </div>



  </div>


  <div class="footer">
    <h3>Modified July 2023</h3>
  </div>
</body>

</html>